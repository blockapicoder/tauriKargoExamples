<!doctype html>
<html lang="fr">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>Azure Speech TTS + Tone.js — Troll / Démon / Ange</title>
<style>
  :root { font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Ubuntu, Cantarell, Noto Sans, Arial; }
  body { margin:0; padding:24px; background:#0b0c10; color:#e5e7eb; }
  h1 { margin:0 0 12px; font-size:20px; }
  .row { display:flex; flex-wrap:wrap; gap:12px; align-items:center; }
  .card { background:#111318; border:1px solid #22252e; border-radius:14px; padding:16px; margin:12px 0; box-shadow:0 4px 14px rgba(0,0,0,.25); }
  label { font-size:13px; opacity:.9; display:flex; flex-direction:column; gap:6px; }
  input[type="range"] { width:230px; }
  input, select, textarea, button { background:#0f1116; border:1px solid #2a2f3a; color:#e5e7eb; border-radius:10px; padding:10px; }
  textarea { width:100%; min-height:120px; }
  button { cursor:pointer; }
  .small { font-size:12px; opacity:.85; }
  .ok { color:#6ee7b7; }
  .warn { color:#ffd166; }
  .muted { opacity:.7; }
</style>
</head>
<body>
<h1>Azure Speech TTS + Tone.js — voix “troll / démon / ange”</h1>
<p class="small muted">Ordre : <b>Init</b> → <b>Parler</b>. (Servez ce fichier via un serveur local, ex. <code>npx http-server</code>.)</p>

<div class="card">
  <div class="row">
    <label>Azure Speech Key <input id="key" size="36" placeholder="YOUR_AZURE_SPEECH_KEY"></label>
    <label>Region <input id="region" size="12" placeholder="westeurope"></label>
    <label>Voix Azure
      <select id="voice">
        <!-- Deux voix FR courantes, changez à volonté -->
        <option value="fr-FR-DeniseNeural">fr-FR-DeniseNeural</option>
        <option value="fr-FR-HenriNeural">fr-FR-HenriNeural</option>
      </select>
    </label>
    <label>Persona
      <select id="persona">
        <option value="narrator">Narrateur</option>
        <option value="troll">Troll</option>
        <option value="demon">Démon</option>
        <option value="angel">Ange</option>
        <option value="robot">Robot</option>
      </select>
    </label>
  </div>
  <div class="row" style="margin-top:10px">
    <button id="init">1) Init audio</button>
    <button id="speak">2) Parler</button>
    <span id="status" class="small"></span>
  </div>
</div>

<div class="card">
  <h2 style="margin:0 0 8px;font-size:16px">Texte</h2>
  <textarea id="text">Bonjour, voyageur… Approche, et écoute la voix des esprits.</textarea>
</div>

<div class="card">
  <h2 style="margin:0 0 8px;font-size:16px">Réglages fins (optionnels)</h2>
  <div class="row">
    <label>Grave/Aigu (pitch-shift en demi-tons)
      <input id="semitones" type="range" min="-12" max="12" value="-3" step="1">
    </label>
    <label>Écho (delay en s)
      <input id="delay" type="range" min="0" max="1" value="0.25" step="0.01">
    </label>
    <label>Feedback
      <input id="feedback" type="range" min="0" max="0.95" value="0.35" step="0.01">
    </label>
    <label>Réverb (decay en s)
      <input id="reverb" type="range" min="0.1" max="8" value="2.5" step="0.1">
    </label>
    <label>Distorsion
      <input id="dist" type="range" min="0" max="0.9" value="0.0" step="0.01">
    </label>
    <label>BitCrusher (bits)
      <input id="bits" type="range" min="1" max="8" value="8" step="1">
    </label>
    <label>Filtre (Hz, 0=off)
      <input id="filter" type="range" min="0" max="12000" value="0" step="50">
    </label>
    <label>Mix (wet)
      <input id="wet" type="range" min="0" max="1" value="0.6" step="0.01">
    </label>
  </div>
</div>

<script type="module">
import * as Tone from "https://cdn.skypack.dev/tone";
import * as sdk  from "https://aka.ms/csspeech/jsbrowserpackageraw"; // Azure Speech SDK (UMD)

const $ = (id)=>document.getElementById(id);
const keyEl=$("key"), regionEl=$("region"), voiceEl=$("voice"), personaEl=$("persona");
const textEl=$("text"), status=$("status");
const semitones=$("semitones"), delayEl=$("delay"), feedback=$("feedback"), reverb=$("reverb");
const dist=$("dist"), bits=$("bits"), filter=$("filter"), wet=$("wet");

let audioReady=false;
async function unlockAudio(){
  if(audioReady) return;
  await Tone.start();
  const ctx = Tone.getContext().rawContext || Tone.getContext().context;
  if (ctx && ctx.state !== "running") await ctx.resume();
  audioReady = true;
}

// ---------- Tone.js graph ----------
let fxBuilt=false;
let pitchShift, echo, verb, distortion, crusher, biquad, wetGain, dryGain, limiter;

async function buildFX(){
  pitchShift  = new Tone.PitchShift({ pitch: parseInt(semitones.value,10) });
  echo        = new Tone.FeedbackDelay({ delayTime: parseFloat(delayEl.value), feedback: parseFloat(feedback.value) });
  verb        = new Tone.Reverb({ decay: parseFloat(reverb.value) });
  await verb.generate();
  distortion  = new Tone.Distortion(parseFloat(dist.value)); // 0..1
  crusher     = new Tone.BitCrusher(parseInt(bits.value,10)); // 1..8
  biquad      = new Tone.Filter({ type:"lowpass", frequency: Math.max(0.001, parseFloat(filter.value)||0) || 20000, rolloff:-24 });
  wetGain     = new Tone.Gain(parseFloat(wet.value));
  dryGain     = new Tone.Gain(1 - parseFloat(wet.value));
  limiter     = new Tone.Limiter(-1);

  // Chaîne wet: pitch -> echo -> reverb -> dist -> crusher -> filter -> wetGain
  pitchShift.chain(echo, verb, distortion, crusher, biquad, wetGain);
  // Sorties
  wetGain.connect(limiter);
  dryGain.connect(limiter);
  limiter.toDestination();

  fxBuilt = true;
}

async function updateFX(){
  pitchShift.pitch = parseInt(semitones.value,10);
  echo.delayTime.value = parseFloat(delayEl.value);
  echo.feedback.value  = parseFloat(feedback.value);
  verb.decay = parseFloat(reverb.value); await verb.generate();
  distortion.distortion = parseFloat(dist.value);
  crusher.bits = parseInt(bits.value,10);
  const f = parseFloat(filter.value);
  biquad.frequency.value = f>0 ? f : 20000; // 0 = désactivé ~ passe-tout
  wetGain.gain.value = parseFloat(wet.value);
  dryGain.gain.value = 1 - parseFloat(wet.value);
}

// ---------- Personae (préréglages SSML + FX) ----------
function personaPreset(p){
  // Valeurs par défaut
  let ssml = {
    style: "general",
    styledegree: "1.0",
    pitch: "0st",
    rate: "0%"
  };
  let fx = {
    semitones: 0, delay: 0.15, feedback: 0.2, reverb: 1.4,
    dist: 0.0, bits: 8, filter: 0, wet: 0.4
  };

  switch(p){
    case "troll":
      ssml = { style:"angry", styledegree:"1.0", pitch:"-4st", rate:"-5%" };
      fx   = { semitones:-5, delay:0.18, feedback:0.25, reverb:1.8, dist:0.08, bits:8, filter:1800, wet:0.6 };
      break;
    case "demon":
      ssml = { style:"angry", styledegree:"1.2", pitch:"-8st", rate:"-8%" };
      fx   = { semitones:-8, delay:0.22, feedback:0.28, reverb:2.6, dist:0.22, bits:6, filter:1200, wet:0.75 };
      break;
    case "angel":
      ssml = { style:"calm", styledegree:"1.0", pitch:"+3st", rate:"+5%" };
      fx   = { semitones:+3, delay:0.35, feedback:0.30, reverb:3.5, dist:0.0, bits:8, filter:0, wet:0.65 };
      break;
    case "robot":
      ssml = { style:"assistant", styledegree:"0.6", pitch:"0st", rate:"0%" };
      fx   = { semitones:0, delay:0.12, feedback:0.18, reverb:0.8, dist:0.05, bits:4, filter:0, wet:0.5 };
      break;
    default: // narrator
      ssml = { style:"general", styledegree:"1.0", pitch:"0st", rate:"+2%" };
      fx   = { semitones:0, delay:0.20, feedback:0.20, reverb:1.6, dist:0.0, bits:8, filter:0, wet:0.45 };
  }
  return { ssml, fx };
}

// Applique les presets FX dans l’UI (et graph)
async function applyFxPreset(p){
  const { fx } = personaPreset(p);
  semitones.value = fx.semitones;
  delayEl.value   = fx.delay;
  feedback.value  = fx.feedback;
  reverb.value    = fx.reverb;
  dist.value      = fx.dist;
  bits.value      = fx.bits;
  filter.value    = fx.filter;
  wet.value       = fx.wet;
  if (fxBuilt) await updateFX();
}

// Génère SSML avec style + prosody (Azure ignorera les styles non supportés par la voix)
function buildSSML(text, voiceName, personaKey){
  const { ssml } = personaPreset(personaKey);
  const esc = (s)=>s.replace(/&/g,"&amp;").replace(/</g,"&lt;").replace(/>/g,"&gt;");
  const t = esc(text);
  return `
<speak version="1.0" xml:lang="fr-FR" xmlns:mstts="https://www.w3.org/2001/mstts">
  <voice name="${voiceName}">
    <mstts:express-as style="${ssml.style}" styledegree="${ssml.styledegree}">
      <prosody pitch="${ssml.pitch}" rate="${ssml.rate}">
        ${t}
      </prosody>
    </mstts:express-as>
  </voice>
</speak>`.trim();
}

// ---------- Azure synthesis (PCM -> WAV -> Tone.Player) ----------
function pcm16ToWavBlob(u8, sampleRate=24000){
  // u8 = Uint8Array PCM16LE mono
  const numFrames = u8.byteLength / 2;
  const blockAlign = 2; // mono 16-bit
  const byteRate = sampleRate * blockAlign;
  const dataSize = u8.byteLength;
  const buffer = new ArrayBuffer(44 + dataSize);
  const dv = new DataView(buffer);
  let p = 0;
  function wUint32(v){ dv.setUint32(p, v, true); p+=4; }
  function wUint16(v){ dv.setUint16(p, v, true); p+=2; }
  function wStr(s){ for(let i=0;i<s.length;i++) dv.setUint8(p++, s.charCodeAt(i)); }

  wStr("RIFF"); wUint32(36 + dataSize); wStr("WAVE");
  wStr("fmt "); wUint32(16); wUint16(1); // PCM
  wUint16(1); // mono
  wUint32(sampleRate); wUint32(byteRate); wUint16(blockAlign); wUint16(16);
  wStr("data"); wUint32(dataSize);
  new Uint8Array(buffer, 44).set(u8);
  return new Blob([buffer], { type: "audio/wav" });
}

async function synthesizeAzureSSML(ssml, key, region){
  const speechConfig = sdk.SpeechConfig.fromSubscription(key, region);
  // Voix/langue configurées via SSML, mais on met par défaut FR :
  speechConfig.speechSynthesisLanguage = "fr-FR";
  // Format PCM 16-bit 24kHz mono (facile à WAVifier)
  speechConfig.setProperty(
    sdk.PropertyId.SpeechServiceConnection_SynthOutputFormat,
    sdk.SpeechSynthesisOutputFormat.Raw24Khz16BitMonoPcm
  );
  const audioOut = sdk.AudioConfig.fromDefaultSpeakerOutput(); // requis par SDK, on n’utilise pas le haut-parleur
  const synth = new sdk.SpeechSynthesizer(speechConfig, audioOut);

  const result = await new Promise((resolve, reject)=>{
    synth.speakSsmlAsync(ssml, r => resolve(r), e => reject(e));
  });
  synth.close();

  if (result.reason !== sdk.ResultReason.SynthesizingAudioCompleted) {
    throw new Error("Synthèse non complétée: " + result.reason);
  }
  const u8 = new Uint8Array(result.audioData); // PCM16LE mono 24kHz
  return pcm16ToWavBlob(u8, 24000);
}

async function playWavBlobWithFX(blob){
  const url = URL.createObjectURL(blob);
  const player = new Tone.Player({ autostart:false });
  // Split dry / wet
  player.fan(dryGain, pitchShift);
  await player.load(url); // charge le buffer
  URL.revokeObjectURL(url);
  player.start();
}

// ---------- UI ----------
$("init").onclick = async ()=>{
  try {
    await unlockAudio();
    if (!fxBuilt) await buildFX();
    await applyFxPreset(personaEl.value);
    // sliders réactifs
    [semitones, delayEl, feedback, reverb, dist, bits, filter, wet].forEach(el => el.oninput = updateFX);
    personaEl.onchange = () => applyFxPreset(personaEl.value);
    status.textContent = "Audio prêt ✅"; status.className = "small ok";
  } catch(e){
    status.textContent = "Init audio : " + (e?.message || e); status.className="small warn";
  }
};

$("speak").onclick = async ()=>{
  try {
    await unlockAudio();
    if (!fxBuilt) await buildFX();
    await updateFX();

    const key = keyEl.value.trim(); const region = regionEl.value.trim();
    if (!key || !region) { status.textContent = "Renseigne key/region Azure."; status.className="small warn"; return; }

    // Construire SSML avec style + prosody selon persona
    const ssml = buildSSML(textEl.value, voiceEl.value, personaEl.value);

    status.textContent = "Synthèse Azure…";
    const wav = await synthesizeAzureSSML(ssml, key, region);

    status.textContent = "Lecture…";
    await playWavBlobWithFX(wav);

    status.textContent = "OK ✅"; status.className = "small ok";
  } catch(e){
    console.error(e);
    status.textContent = "Erreur : " + (e?.message || e); status.className="small warn";
  }
};
</script>
</body>
</html>
